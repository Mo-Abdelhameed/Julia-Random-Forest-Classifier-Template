{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "using Suppressor\n",
    "@suppress begin\n",
    "    using DataFrames, CSV, Random, Statistics, Serialization, LazyJSON, StatsBase, DecisionTree, ScientificTypes, MLJ\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES \n",
    "ROOT_DIR = dirname(pwd())\n",
    "MODEL_INPUTS_OUTPUTS = joinpath(ROOT_DIR, \"model_inputs_outputs\")\n",
    "INPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = joinpath(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = joinpath(INPUT_DIR, \"data\")\n",
    "OUTPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"outputs\")\n",
    "TRAIN_DIR = joinpath(DATA_DIR, \"training\")\n",
    "TEST_DIR = joinpath(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = joinpath(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = joinpath(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"ohe.jld2\")\n",
    "PREDICTOR_DIR_PATH = joinpath(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = joinpath(PREDICTOR_DIR_PATH, \"predictor.ser\")\n",
    "IMPUTATION_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"imputation.ser\")\n",
    "TOP_CATEGORIES = joinpath(MODEL_ARTIFACTS_PATH, \"top_categories.ser\")\n",
    "PREDICTIONS_DIR = joinpath(OUTPUT_DIR, \"predictions\")\n",
    "PREDICTIONS_FILE = joinpath(PREDICTIONS_DIR, \"predictions.csv\")\n",
    "TARGET_LEVELS = joinpath(MODEL_ARTIFACTS_PATH, \"target_levels.ser\")\n",
    "\n",
    "\n",
    "\n",
    "if !isdir(MODEL_ARTIFACTS_PATH)\n",
    "    mkdir(MODEL_ARTIFACTS_PATH)\n",
    "end\n",
    "if !isdir(PREDICTOR_DIR_PATH)\n",
    "    mkdir(PREDICTOR_DIR_PATH)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element LazyJSON.Array{Nothing, String}:\n",
       " 0\n",
       " 1\n",
       " 2"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a schema from a JSON file and extracting features\n",
    "file_name = first(filter(x -> endswith(x, \"json\"), readdir(INPUT_SCHEMA_DIR)))\n",
    "schema_path = joinpath(INPUT_SCHEMA_DIR, file_name)\n",
    "schema_string = read(schema_path, String)  # Read file content as a string\n",
    "schema = LazyJSON.parse(schema_string)\n",
    "features = schema[\"features\"]\n",
    "\n",
    "# Identifying numeric, categorical, and nullable features\n",
    "numeric_features = String[]\n",
    "categorical_features = String[]\n",
    "nullable_features = String[]\n",
    "\n",
    "for f in features\n",
    "    if f[\"dataType\"] == \"CATEGORICAL\"\n",
    "        push!(categorical_features, f[\"name\"])\n",
    "    else\n",
    "        push!(numeric_features, f[\"name\"])\n",
    "    end\n",
    "    if f[\"nullable\"]\n",
    "        push!(nullable_features, f[\"name\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Extracting ID and target features\n",
    "id_feature = schema[\"id\"][\"name\"]\n",
    "target_feature = schema[\"target\"][\"name\"]\n",
    "target_classes = schema[\"target\"][\"classes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>40 rows × 3 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>color</th><th>number</th></tr><tr><th></th><th title=\"String7\">String7</th><th title=\"Union{Missing, String7}\">String7?</th><th title=\"Union{Missing, Float64}\">Float64?</th></tr></thead><tbody><tr><th>1</th><td>WL7HCI</td><td>Blue</td><td><em>missing</em></td></tr><tr><th>2</th><td>3WY4QM</td><td>Red</td><td>45.0</td></tr><tr><th>3</th><td>Z3IWZH</td><td>Green</td><td>40.0</td></tr><tr><th>4</th><td>2K5IWJ</td><td>Green</td><td>12.0</td></tr><tr><th>5</th><td>YH1373</td><td>Green</td><td>80.0</td></tr><tr><th>6</th><td>AQGWOL</td><td>Green</td><td>47.0</td></tr><tr><th>7</th><td>MPPCVX</td><td>Blue</td><td>90.0</td></tr><tr><th>8</th><td>8UI6PP</td><td><em>missing</em></td><td>55.0</td></tr><tr><th>9</th><td>DJLE12</td><td>Red</td><td>74.0</td></tr><tr><th>10</th><td>ZBAXHT</td><td>Red</td><td>57.0</td></tr><tr><th>11</th><td>NLYMTV</td><td>Blue</td><td>96.0</td></tr><tr><th>12</th><td>1H3ZZ4</td><td>Green</td><td><em>missing</em></td></tr><tr><th>13</th><td>LGBZNY</td><td>Blue</td><td>34.0</td></tr><tr><th>14</th><td>BMUDU8</td><td>Green</td><td>51.0</td></tr><tr><th>15</th><td>OUKR4T</td><td>Red</td><td><em>missing</em></td></tr><tr><th>16</th><td>DK5Y3S</td><td>Blue</td><td>77.0</td></tr><tr><th>17</th><td>O6CPO2</td><td>Blue</td><td>25.0</td></tr><tr><th>18</th><td>GNB733</td><td>Blue</td><td>86.0</td></tr><tr><th>19</th><td>17K9LP</td><td>Blue</td><td>84.0</td></tr><tr><th>20</th><td>DM8A7V</td><td><em>missing</em></td><td>55.0</td></tr><tr><th>21</th><td>207XWX</td><td>Green</td><td><em>missing</em></td></tr><tr><th>22</th><td>XXLPBF</td><td>Blue</td><td><em>missing</em></td></tr><tr><th>23</th><td>0GGTMB</td><td>Red</td><td>55.0</td></tr><tr><th>24</th><td>98FI1I</td><td>Red</td><td>82.0</td></tr><tr><th>25</th><td>E6IWI1</td><td>Blue</td><td>52.0</td></tr><tr><th>26</th><td>WXR4S6</td><td>Blue</td><td>87.0</td></tr><tr><th>27</th><td>6UJVQQ</td><td>Blue</td><td>81.0</td></tr><tr><th>28</th><td>1IJ1I3</td><td>Blue</td><td>95.0</td></tr><tr><th>29</th><td>T69760</td><td>Red</td><td>15.0</td></tr><tr><th>30</th><td>Y0KH6F</td><td>Blue</td><td>58.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& id & color & number\\\\\n",
       "\t\\hline\n",
       "\t& String7 & String7? & Float64?\\\\\n",
       "\t\\hline\n",
       "\t1 & WL7HCI & Blue & \\emph{missing} \\\\\n",
       "\t2 & 3WY4QM & Red & 45.0 \\\\\n",
       "\t3 & Z3IWZH & Green & 40.0 \\\\\n",
       "\t4 & 2K5IWJ & Green & 12.0 \\\\\n",
       "\t5 & YH1373 & Green & 80.0 \\\\\n",
       "\t6 & AQGWOL & Green & 47.0 \\\\\n",
       "\t7 & MPPCVX & Blue & 90.0 \\\\\n",
       "\t8 & 8UI6PP & \\emph{missing} & 55.0 \\\\\n",
       "\t9 & DJLE12 & Red & 74.0 \\\\\n",
       "\t10 & ZBAXHT & Red & 57.0 \\\\\n",
       "\t11 & NLYMTV & Blue & 96.0 \\\\\n",
       "\t12 & 1H3ZZ4 & Green & \\emph{missing} \\\\\n",
       "\t13 & LGBZNY & Blue & 34.0 \\\\\n",
       "\t14 & BMUDU8 & Green & 51.0 \\\\\n",
       "\t15 & OUKR4T & Red & \\emph{missing} \\\\\n",
       "\t16 & DK5Y3S & Blue & 77.0 \\\\\n",
       "\t17 & O6CPO2 & Blue & 25.0 \\\\\n",
       "\t18 & GNB733 & Blue & 86.0 \\\\\n",
       "\t19 & 17K9LP & Blue & 84.0 \\\\\n",
       "\t20 & DM8A7V & \\emph{missing} & 55.0 \\\\\n",
       "\t21 & 207XWX & Green & \\emph{missing} \\\\\n",
       "\t22 & XXLPBF & Blue & \\emph{missing} \\\\\n",
       "\t23 & 0GGTMB & Red & 55.0 \\\\\n",
       "\t24 & 98FI1I & Red & 82.0 \\\\\n",
       "\t25 & E6IWI1 & Blue & 52.0 \\\\\n",
       "\t26 & WXR4S6 & Blue & 87.0 \\\\\n",
       "\t27 & 6UJVQQ & Blue & 81.0 \\\\\n",
       "\t28 & 1IJ1I3 & Blue & 95.0 \\\\\n",
       "\t29 & T69760 & Red & 15.0 \\\\\n",
       "\t30 & Y0KH6F & Blue & 58.0 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m40×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m id      \u001b[0m\u001b[1m color    \u001b[0m\u001b[1m number    \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String7 \u001b[0m\u001b[90m String7? \u001b[0m\u001b[90m Float64?  \u001b[0m\n",
       "─────┼──────────────────────────────\n",
       "   1 │ WL7HCI   Blue     \u001b[90m missing   \u001b[0m\n",
       "   2 │ 3WY4QM   Red            45.0\n",
       "   3 │ Z3IWZH   Green          40.0\n",
       "   4 │ 2K5IWJ   Green          12.0\n",
       "   5 │ YH1373   Green          80.0\n",
       "   6 │ AQGWOL   Green          47.0\n",
       "   7 │ MPPCVX   Blue           90.0\n",
       "   8 │ 8UI6PP  \u001b[90m missing  \u001b[0m      55.0\n",
       "   9 │ DJLE12   Red            74.0\n",
       "  10 │ ZBAXHT   Red            57.0\n",
       "  11 │ NLYMTV   Blue           96.0\n",
       "  ⋮  │    ⋮        ⋮          ⋮\n",
       "  31 │ YE2RF5  \u001b[90m missing  \u001b[0m      92.0\n",
       "  32 │ 6NXPXX  \u001b[90m missing  \u001b[0m      18.0\n",
       "  33 │ Z7F6NZ   Blue           12.0\n",
       "  34 │ OBLULI   Red            26.0\n",
       "  35 │ CX4YHE   Blue           35.0\n",
       "  36 │ S75RO0   Red      \u001b[90m missing   \u001b[0m\n",
       "  37 │ ROWCZB   Green          28.0\n",
       "  38 │ J0XQFV   Blue           82.0\n",
       "  39 │ UUZLUF   Blue           83.0\n",
       "  40 │ 16BD07   Blue           64.0\n",
       "\u001b[36m                     19 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = filter(x -> occursin(\".csv\", x), readdir(TEST_DIR))[1]\n",
    "file_path = joinpath(TEST_DIR, file_name)\n",
    "df = DataFrame(CSV.File(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Note that when we work with testing data, we have to impute using the same values learned during training. This is to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>40 rows × 2 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>color</th><th>number</th></tr><tr><th></th><th title=\"String7\">String7</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>Blue</td><td>45.0</td></tr><tr><th>2</th><td>Red</td><td>45.0</td></tr><tr><th>3</th><td>Green</td><td>40.0</td></tr><tr><th>4</th><td>Green</td><td>12.0</td></tr><tr><th>5</th><td>Green</td><td>80.0</td></tr><tr><th>6</th><td>Green</td><td>47.0</td></tr><tr><th>7</th><td>Blue</td><td>90.0</td></tr><tr><th>8</th><td>Blue</td><td>55.0</td></tr><tr><th>9</th><td>Red</td><td>74.0</td></tr><tr><th>10</th><td>Red</td><td>57.0</td></tr><tr><th>11</th><td>Blue</td><td>96.0</td></tr><tr><th>12</th><td>Green</td><td>45.0</td></tr><tr><th>13</th><td>Blue</td><td>34.0</td></tr><tr><th>14</th><td>Green</td><td>51.0</td></tr><tr><th>15</th><td>Red</td><td>45.0</td></tr><tr><th>16</th><td>Blue</td><td>77.0</td></tr><tr><th>17</th><td>Blue</td><td>25.0</td></tr><tr><th>18</th><td>Blue</td><td>86.0</td></tr><tr><th>19</th><td>Blue</td><td>84.0</td></tr><tr><th>20</th><td>Blue</td><td>55.0</td></tr><tr><th>21</th><td>Green</td><td>45.0</td></tr><tr><th>22</th><td>Blue</td><td>45.0</td></tr><tr><th>23</th><td>Red</td><td>55.0</td></tr><tr><th>24</th><td>Red</td><td>82.0</td></tr><tr><th>25</th><td>Blue</td><td>52.0</td></tr><tr><th>26</th><td>Blue</td><td>87.0</td></tr><tr><th>27</th><td>Blue</td><td>81.0</td></tr><tr><th>28</th><td>Blue</td><td>95.0</td></tr><tr><th>29</th><td>Red</td><td>15.0</td></tr><tr><th>30</th><td>Blue</td><td>58.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& color & number\\\\\n",
       "\t\\hline\n",
       "\t& String7 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & Blue & 45.0 \\\\\n",
       "\t2 & Red & 45.0 \\\\\n",
       "\t3 & Green & 40.0 \\\\\n",
       "\t4 & Green & 12.0 \\\\\n",
       "\t5 & Green & 80.0 \\\\\n",
       "\t6 & Green & 47.0 \\\\\n",
       "\t7 & Blue & 90.0 \\\\\n",
       "\t8 & Blue & 55.0 \\\\\n",
       "\t9 & Red & 74.0 \\\\\n",
       "\t10 & Red & 57.0 \\\\\n",
       "\t11 & Blue & 96.0 \\\\\n",
       "\t12 & Green & 45.0 \\\\\n",
       "\t13 & Blue & 34.0 \\\\\n",
       "\t14 & Green & 51.0 \\\\\n",
       "\t15 & Red & 45.0 \\\\\n",
       "\t16 & Blue & 77.0 \\\\\n",
       "\t17 & Blue & 25.0 \\\\\n",
       "\t18 & Blue & 86.0 \\\\\n",
       "\t19 & Blue & 84.0 \\\\\n",
       "\t20 & Blue & 55.0 \\\\\n",
       "\t21 & Green & 45.0 \\\\\n",
       "\t22 & Blue & 45.0 \\\\\n",
       "\t23 & Red & 55.0 \\\\\n",
       "\t24 & Red & 82.0 \\\\\n",
       "\t25 & Blue & 52.0 \\\\\n",
       "\t26 & Blue & 87.0 \\\\\n",
       "\t27 & Blue & 81.0 \\\\\n",
       "\t28 & Blue & 95.0 \\\\\n",
       "\t29 & Red & 15.0 \\\\\n",
       "\t30 & Blue & 58.0 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m40×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m color   \u001b[0m\u001b[1m number  \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String7 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────\n",
       "   1 │ Blue        45.0\n",
       "   2 │ Red         45.0\n",
       "   3 │ Green       40.0\n",
       "   4 │ Green       12.0\n",
       "   5 │ Green       80.0\n",
       "   6 │ Green       47.0\n",
       "   7 │ Blue        90.0\n",
       "   8 │ Blue        55.0\n",
       "   9 │ Red         74.0\n",
       "  10 │ Red         57.0\n",
       "  11 │ Blue        96.0\n",
       "  ⋮  │    ⋮        ⋮\n",
       "  31 │ Blue        92.0\n",
       "  32 │ Blue        18.0\n",
       "  33 │ Blue        12.0\n",
       "  34 │ Red         26.0\n",
       "  35 │ Blue        35.0\n",
       "  36 │ Red         45.0\n",
       "  37 │ Green       28.0\n",
       "  38 │ Blue        82.0\n",
       "  39 │ Blue        83.0\n",
       "  40 │ Blue        64.0\n",
       "\u001b[36m         19 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputation_values = open(deserialize, IMPUTATION_FILE)\n",
    "for column in nullable_features\n",
    "    df[!, Symbol(column)] .= coalesce.(df[!, Symbol(column)], get(imputation_values, string(column), missing))\n",
    "end\n",
    "\n",
    "# Saving the id column in a different variable\n",
    "ids = df[!, Symbol(id_feature)]\n",
    "\n",
    "# Dropping the id from the DataFrame\n",
    "select!(df, Not([Symbol(id_feature)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "We encode the data using the same encoder that we saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_top_categories = open(deserialize, TOP_CATEGORIES)\n",
    "\n",
    "# Function to one-hot encode only the top 3 categories\n",
    "function one_hot_top_categories!(df, top_categories)\n",
    "    for (feature, top_cats) in top_categories\n",
    "        if length(top_cats) == 2  # Handle the binary case\n",
    "            # Assuming the first category in top_cats is treated as 'true'\n",
    "            new_col_name = \"$(feature)_binary\"\n",
    "            df[!, new_col_name] = df[!, feature] .== top_cats[1]\n",
    "        else  # Handle the general case\n",
    "            for cat in top_cats\n",
    "                new_col_name = \"$(feature)_$(cat)\"\n",
    "                df[!, new_col_name] = df[!, feature] .== cat\n",
    "            end\n",
    "        end\n",
    "        select!(df, Not(Symbol(feature)))  # Drop the original feature column\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "one_hot_top_categories!(df, loaded_top_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions & Creating Predictions DataFrame\n",
    "Using the model saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching String(::LazyJSON.Number{String})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  String(\u001b[91m::T\u001b[39m) where T<:InlineString\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mInlineStrings\u001b[39m \u001b[90m~/.julia/packages/InlineStrings/rlLZO/src/\u001b[39m\u001b[90m\u001b[4mInlineStrings.jl:133\u001b[24m\u001b[39m\n\u001b[0m  String(\u001b[91m::CategoricalArrays.CategoricalValue{<:AbstractString}\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mCategoricalArrays\u001b[39m \u001b[90m~/.julia/packages/CategoricalArrays/0yLZN/src/\u001b[39m\u001b[90m\u001b[4mvalue.jl:116\u001b[24m\u001b[39m\n\u001b[0m  String(\u001b[91m::WeakRefStrings.WeakRefString\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[36mWeakRefStrings\u001b[39m \u001b[90m~/.julia/packages/WeakRefStrings/31nkb/src/\u001b[39m\u001b[90m\u001b[4mWeakRefStrings.jl:82\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching String(::LazyJSON.Number{String})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  String(\u001b[91m::T\u001b[39m) where T<:InlineString\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mInlineStrings\u001b[39m \u001b[90m~/.julia/packages/InlineStrings/rlLZO/src/\u001b[39m\u001b[90m\u001b[4mInlineStrings.jl:133\u001b[24m\u001b[39m\n\u001b[0m  String(\u001b[91m::CategoricalArrays.CategoricalValue{<:AbstractString}\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mCategoricalArrays\u001b[39m \u001b[90m~/.julia/packages/CategoricalArrays/0yLZN/src/\u001b[39m\u001b[90m\u001b[4mvalue.jl:116\u001b[24m\u001b[39m\n\u001b[0m  String(\u001b[91m::WeakRefStrings.WeakRefString\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[36mWeakRefStrings\u001b[39m \u001b[90m~/.julia/packages/WeakRefStrings/31nkb/src/\u001b[39m\u001b[90m\u001b[4mWeakRefStrings.jl:82\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
      "",
      "Stacktrace:",
      " [1] _broadcast_getindex_evalf",
      "   @ ./broadcast.jl:683 [inlined]",
      " [2] _broadcast_getindex",
      "   @ ./broadcast.jl:656 [inlined]",
      " [3] getindex",
      "   @ ./broadcast.jl:610 [inlined]",
      " [4] copy",
      "   @ ./broadcast.jl:912 [inlined]",
      " [5] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, Type{String}, Tuple{LazyJSON.Array{Nothing, String}}})",
      "   @ Base.Broadcast ./broadcast.jl:873",
      " [6] top-level scope",
      "   @ In[115]:2"
     ]
    }
   ],
   "source": [
    "model = open(deserialize, PREDICTOR_FILE_PATH)\n",
    "predictions = apply_forest_proba(model, Matrix(df), [string(item) for item in target_classes])\n",
    "predictions = DataFrame(predictions, Symbol.(target_classes))\n",
    "insertcols!(predictions, 1, id_feature => ids)\n",
    "CSV.write(PREDICTIONS_FILE, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
