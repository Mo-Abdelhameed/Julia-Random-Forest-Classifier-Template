{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES\n",
    "using Suppressor\n",
    "@suppress begin\n",
    "    using DataFrames, CSV, Random, Statistics, Serialization, LazyJSON, StatsBase, DecisionTree, ScientificTypes, MLJ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES\n",
    "ROOT_DIR = dirname(pwd())\n",
    "MODEL_INPUTS_OUTPUTS = joinpath(ROOT_DIR, \"model_inputs_outputs\")\n",
    "INPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = joinpath(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = joinpath(INPUT_DIR, \"data\")\n",
    "TRAIN_DIR = joinpath(DATA_DIR, \"training\")\n",
    "TEST_DIR = joinpath(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = joinpath(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = joinpath(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"ohe.jld2\")\n",
    "PREDICTOR_DIR_PATH = joinpath(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = joinpath(PREDICTOR_DIR_PATH, \"predictor.ser\")\n",
    "IMPUTATION_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"imputation.ser\")\n",
    "TOP_CATEGORIES = joinpath(MODEL_ARTIFACTS_PATH, \"top_categories.ser\")\n",
    "TARGET_LEVELS = joinpath(MODEL_ARTIFACTS_PATH, \"target_levels.ser\")\n",
    "\n",
    "if !isdir(MODEL_ARTIFACTS_PATH)\n",
    "    mkdir(MODEL_ARTIFACTS_PATH)\n",
    "end\n",
    "if !isdir(PREDICTOR_DIR_PATH)\n",
    "    mkdir(PREDICTOR_DIR_PATH)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema\n",
    "The schema contains metadata about the datasets. We will use the scehma to get information about the type of each feature (NUMERIC or CATEGORICAL) and the id and target features, this will be helpful in preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"disease\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a schema from a JSON file and extracting features\n",
    "file_name = first(filter(x -> endswith(x, \"json\"), readdir(INPUT_SCHEMA_DIR)))\n",
    "schema_path = joinpath(INPUT_SCHEMA_DIR, file_name)\n",
    "schema_string = read(schema_path, String)\n",
    "schema = LazyJSON.parse(schema_string)\n",
    "\n",
    "features = schema[\"features\"]\n",
    "numeric_features = String[]\n",
    "categorical_features = String[]\n",
    "nullable_features = String[]\n",
    "\n",
    "for f in features\n",
    "    if f[\"dataType\"] == \"CATEGORICAL\"\n",
    "        push!(categorical_features, f[\"name\"])\n",
    "    else\n",
    "        push!(numeric_features, f[\"name\"])\n",
    "    end\n",
    "    if f[\"nullable\"]\n",
    "        push!(nullable_features, f[\"name\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Extracting ID and target features\n",
    "id_feature = schema[\"id\"][\"name\"]\n",
    "target_feature = schema[\"target\"][\"name\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>546 rows × 37 columns (omitted printing of 30 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>date</th><th>plant-stand</th><th>precip</th><th>temp</th><th>hail</th><th>crop-hist</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Union{Missing, String15}\">String15?</th><th title=\"Union{Missing, String15}\">String15?</th><th title=\"Union{Missing, String15}\">String15?</th><th title=\"Union{Missing, String15}\">String15?</th><th title=\"Union{Missing, String7}\">String7?</th><th title=\"Union{Missing, String31}\">String31?</th></tr></thead><tbody><tr><th>1</th><td>268</td><td>september</td><td> normal</td><td> gt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>2</th><td>204</td><td>july</td><td> lt-normal</td><td> norm</td><td> gt-norm</td><td> yes</td><td> same-lst-sev-yrs</td></tr><tr><th>3</th><td>664</td><td>september</td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td></tr><tr><th>4</th><td>391</td><td>september</td><td> normal</td><td> lt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>5</th><td>434</td><td>september</td><td> normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>6</th><td>218</td><td>october</td><td> normal</td><td> gt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-sev-yrs</td></tr><tr><th>7</th><td>146</td><td>july</td><td> normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> same-lst-sev-yrs</td></tr><tr><th>8</th><td>509</td><td>september</td><td> normal</td><td> gt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-yr</td></tr><tr><th>9</th><td>262</td><td>august</td><td> normal</td><td> gt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-yr</td></tr><tr><th>10</th><td>508</td><td>july</td><td> normal</td><td> gt-norm</td><td> lt-norm</td><td> yes</td><td> same-lst-sev-yrs</td></tr><tr><th>11</th><td>486</td><td>july</td><td> normal</td><td> norm</td><td> norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>12</th><td>406</td><td>september</td><td> normal</td><td> lt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>13</th><td>355</td><td>july</td><td> lt-normal</td><td> norm</td><td> norm</td><td><em>missing</em></td><td> same-lst-sev-yrs</td></tr><tr><th>14</th><td>350</td><td>august</td><td> lt-normal</td><td> norm</td><td> norm</td><td><em>missing</em></td><td> same-lst-sev-yrs</td></tr><tr><th>15</th><td>13</td><td>october</td><td> normal</td><td> lt-norm</td><td> norm</td><td> no</td><td> same-lst-sev-yrs</td></tr><tr><th>16</th><td>398</td><td>august</td><td> lt-normal</td><td> lt-norm</td><td> lt-norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>17</th><td>248</td><td>september</td><td> normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> same-lst-sev-yrs</td></tr><tr><th>18</th><td>19</td><td>september</td><td> normal</td><td> lt-norm</td><td> gt-norm</td><td> no</td><td> same-lst-two-yrs</td></tr><tr><th>19</th><td>582</td><td>august</td><td> normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> same-lst-yr</td></tr><tr><th>20</th><td>145</td><td>june</td><td> normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>21</th><td>581</td><td>august</td><td> lt-normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> same-lst-sev-yrs</td></tr><tr><th>22</th><td>254</td><td>october</td><td> lt-normal</td><td> gt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-sev-yrs</td></tr><tr><th>23</th><td>666</td><td>may</td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td></tr><tr><th>24</th><td>242</td><td>august</td><td> normal</td><td> gt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-yr</td></tr><tr><th>25</th><td>11</td><td>august</td><td> normal</td><td> lt-norm</td><td> norm</td><td> no</td><td> same-lst-yr</td></tr><tr><th>26</th><td>392</td><td>september</td><td> normal</td><td> lt-norm</td><td> norm</td><td> yes</td><td> same-lst-yr</td></tr><tr><th>27</th><td>236</td><td>august</td><td> lt-normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> diff-lst-year</td></tr><tr><th>28</th><td>169</td><td>september</td><td> lt-normal</td><td> norm</td><td> norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>29</th><td>640</td><td>september</td><td> normal</td><td> gt-norm</td><td> gt-norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>30</th><td>140</td><td>august</td><td> normal</td><td> gt-norm</td><td> norm</td><td> yes</td><td> same-lst-two-yrs</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& id & date & plant-stand & precip & temp & hail & crop-hist & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String15? & String15? & String15? & String15? & String7? & String31? & \\\\\n",
       "\t\\hline\n",
       "\t1 & 268 & september &  normal &  gt-norm &  gt-norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t2 & 204 & july &  lt-normal &  norm &  gt-norm &  yes &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t3 & 664 & september & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & $\\dots$ \\\\\n",
       "\t4 & 391 & september &  normal &  lt-norm &  gt-norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t5 & 434 & september &  normal &  gt-norm &  norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t6 & 218 & october &  normal &  gt-norm &  gt-norm &  yes &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t7 & 146 & july &  normal &  gt-norm &  norm &  yes &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t8 & 509 & september &  normal &  gt-norm &  gt-norm &  yes &  same-lst-yr & $\\dots$ \\\\\n",
       "\t9 & 262 & august &  normal &  gt-norm &  gt-norm &  yes &  same-lst-yr & $\\dots$ \\\\\n",
       "\t10 & 508 & july &  normal &  gt-norm &  lt-norm &  yes &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t11 & 486 & july &  normal &  norm &  norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t12 & 406 & september &  normal &  lt-norm &  gt-norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t13 & 355 & july &  lt-normal &  norm &  norm & \\emph{missing} &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t14 & 350 & august &  lt-normal &  norm &  norm & \\emph{missing} &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t15 & 13 & october &  normal &  lt-norm &  norm &  no &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t16 & 398 & august &  lt-normal &  lt-norm &  lt-norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t17 & 248 & september &  normal &  gt-norm &  norm &  yes &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t18 & 19 & september &  normal &  lt-norm &  gt-norm &  no &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t19 & 582 & august &  normal &  gt-norm &  norm &  yes &  same-lst-yr & $\\dots$ \\\\\n",
       "\t20 & 145 & june &  normal &  gt-norm &  norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t21 & 581 & august &  lt-normal &  gt-norm &  norm &  yes &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t22 & 254 & october &  lt-normal &  gt-norm &  gt-norm &  yes &  same-lst-sev-yrs & $\\dots$ \\\\\n",
       "\t23 & 666 & may & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & $\\dots$ \\\\\n",
       "\t24 & 242 & august &  normal &  gt-norm &  gt-norm &  yes &  same-lst-yr & $\\dots$ \\\\\n",
       "\t25 & 11 & august &  normal &  lt-norm &  norm &  no &  same-lst-yr & $\\dots$ \\\\\n",
       "\t26 & 392 & september &  normal &  lt-norm &  norm &  yes &  same-lst-yr & $\\dots$ \\\\\n",
       "\t27 & 236 & august &  lt-normal &  gt-norm &  norm &  yes &  diff-lst-year & $\\dots$ \\\\\n",
       "\t28 & 169 & september &  lt-normal &  norm &  norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t29 & 640 & september &  normal &  gt-norm &  gt-norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t30 & 140 & august &  normal &  gt-norm &  norm &  yes &  same-lst-two-yrs & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m546×37 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m id    \u001b[0m\u001b[1m date      \u001b[0m\u001b[1m plant-stand \u001b[0m\u001b[1m precip    \u001b[0m\u001b[1m temp      \u001b[0m\u001b[1m hail     \u001b[0m\u001b[1m crop-his\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m String15? \u001b[0m\u001b[90m String15?   \u001b[0m\u001b[90m String15? \u001b[0m\u001b[90m String15? \u001b[0m\u001b[90m String7? \u001b[0m\u001b[90m String31\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │   268  september   normal       gt-norm    gt-norm    yes       same-ls ⋯\n",
       "   2 │   204  july        lt-normal    norm       gt-norm    yes       same-ls\n",
       "   3 │   664  september \u001b[90m missing     \u001b[0m\u001b[90m missing   \u001b[0m\u001b[90m missing   \u001b[0m\u001b[90m missing  \u001b[0m\u001b[90m missing\u001b[0m\n",
       "   4 │   391  september   normal       lt-norm    gt-norm    yes       same-ls\n",
       "   5 │   434  september   normal       gt-norm    norm       yes       same-ls ⋯\n",
       "   6 │   218  october     normal       gt-norm    gt-norm    yes       same-ls\n",
       "   7 │   146  july        normal       gt-norm    norm       yes       same-ls\n",
       "   8 │   509  september   normal       gt-norm    gt-norm    yes       same-ls\n",
       "   9 │   262  august      normal       gt-norm    gt-norm    yes       same-ls ⋯\n",
       "  10 │   508  july        normal       gt-norm    lt-norm    yes       same-ls\n",
       "  11 │   486  july        normal       norm       norm       yes       same-ls\n",
       "  ⋮  │   ⋮        ⋮           ⋮           ⋮          ⋮         ⋮               ⋱\n",
       " 537 │   388  september   normal       lt-norm    norm       no        same-ls\n",
       " 538 │   359  july        lt-normal    norm       norm     \u001b[90m missing  \u001b[0m  same-ls ⋯\n",
       " 539 │   346  august      lt-normal    norm       gt-norm  \u001b[90m missing  \u001b[0m  same-ls\n",
       " 540 │   253  september   lt-normal    gt-norm    norm       yes       same-ls\n",
       " 541 │   441  june        normal       norm       norm       yes       same-ls\n",
       " 542 │    32  june        lt-normal    gt-norm    gt-norm  \u001b[90m missing  \u001b[0m  same-ls ⋯\n",
       " 543 │   573  september   normal       gt-norm    gt-norm    yes       same-ls\n",
       " 544 │   511  april       normal       gt-norm    norm       yes       diff-ls\n",
       " 545 │   376  june        lt-normal    gt-norm    norm     \u001b[90m missing  \u001b[0m  same-ls\n",
       " 546 │   343  april       lt-normal    gt-norm    gt-norm  \u001b[90m missing  \u001b[0m  same-ls ⋯\n",
       "\u001b[36m                                                 31 columns and 525 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "file_name = first(filter(x -> endswith(x, \".csv\"), readdir(TRAIN_DIR)))\n",
    "file_path = joinpath(TRAIN_DIR, file_name)\n",
    "train_df = DataFrame(CSV.File(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing is very important before training the model, as the data may contain missing values in some cells. Moreover, most of the learning algorithms cannot work with categorical data, thus the data has to be encoded.\n",
    "\n",
    "In this section we will impute the missing values and encode the categorical features. Afterwards the data will be ready to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing missing data\n",
    "> The median value will be used to impute missing values of the numeric features and the mode will be used to impute categorical features.\n",
    "\n",
    "##### You can add your own preprocessing steps such as:\n",
    "<ul>\n",
    "<li>Normalization</li> <br>\n",
    "<li>Outlier removal</li><br>\n",
    "<li>Dropping or adding features</li><br>\n",
    "</ul>\n",
    "\n",
    "### Important note:\n",
    "<p> \n",
    "Saving the values used for imputation during training step is crucial. These values will be used to impute missing data in the testing set. This is very important to avoid the well known problem of data leakage. During testing, you should not make any assumptions about the data in hand, alternatively anything needed during the testing phase should be learned from the training phase. This is why we are creating a dictionary of values used during training to reuse these values during testing.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing data\n",
    "imputation_values = Dict{String, Any}()\n",
    "\n",
    "for column in nullable_features\n",
    "    if column in numeric_features\n",
    "        value = median(skipmissing(train_df[!, column]))\n",
    "    else\n",
    "        value = mode(skipmissing(train_df[!, column]))\n",
    "    end\n",
    "    train_df[!, column] = coalesce.(train_df[!, column], value)\n",
    "    imputation_values[column] = value\n",
    "end\n",
    "\n",
    "# Serialize the imputation_values dictionary to a binary file\n",
    "open(IMPUTATION_FILE, \"w\") do io\n",
    "    serialize(io, imputation_values)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding Categorical features\n",
    "<p>\n",
    "The id column is just an identifier for the training example, so we will exclude it during the encoding phase.<br>\n",
    "Target feature will be label encoded in the next step.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 35 entries:\n",
       "  \"leaf-mild\"      => String15[\" absent\", \" lower-surf\", \" upper-surf\"]\n",
       "  \"seed-discolor\"  => String15[\" absent\", \" present\"]\n",
       "  \"fruit-pods\"     => String15[\" norm\", \" diseased\", \" dna\"]\n",
       "  \"roots\"          => String15[\" norm\", \" rotted\", \" galls-cysts\"]\n",
       "  \"precip\"         => String15[\" gt-norm\", \" norm\", \" lt-norm\"]\n",
       "  \"lodging\"        => String7[\" yes\", \" no\"]\n",
       "  \"stem-cankers\"   => String15[\" absent\", \" above-sec-nde\", \" below-soil\"]\n",
       "  \"leaf-malf\"      => String15[\" absent\", \" present\"]\n",
       "  \"temp\"           => String15[\" norm\", \" gt-norm\", \" lt-norm\"]\n",
       "  \"area-damaged\"   => String15[\" low-areas\", \" whole-field\", \" upper-areas\"]\n",
       "  \"shriveling\"     => String15[\" absent\", \" present\"]\n",
       "  \"crop-hist\"      => String31[\" same-lst-two-yrs\", \" same-lst-sev-yrs\", \" same…\n",
       "  \"mold-growth\"    => String15[\" absent\", \" present\"]\n",
       "  \"mycelium\"       => String15[\" absent\", \" present\"]\n",
       "  \"seed-size\"      => String15[\" norm\", \" lt-norm\"]\n",
       "  \"hail\"           => String7[\" yes\", \" no\"]\n",
       "  \"plant-growth\"   => String7[\" norm\", \" abnorm\"]\n",
       "  \"severity\"       => String15[\" pot-severe\", \" minor\", \" severe\"]\n",
       "  \"date\"           => String15[\"september\", \"august\", \"july\"]\n",
       "  \"plant-stand\"    => String15[\" normal\", \" lt-normal\"]\n",
       "  \"leafspot-size\"  => String7[\" gt-1/8\", \" dna\", \" lt-1/8\"]\n",
       "  \"leaves\"         => String7[\" abnorm\", \" norm\"]\n",
       "  \"seed-tmt\"       => String15[\" none\", \" fungicide\", \" other\"]\n",
       "  \"leafspots-marg\" => String15[\" w-s-marg\", \" dna\", \" no-w-s-marg\"]\n",
       "  \" stem\"          => String7[\" abnorm\", \" norm\"]\n",
       "  ⋮                => ⋮"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the target column in a variable\n",
    "target = train_df[:, target_feature]\n",
    "\n",
    "# Drop the target column from the dataframe\n",
    "select!(train_df, Not(target_feature))\n",
    "\n",
    "# Function to get top 3 categories\n",
    "function get_top_categories(df, features, n=3)\n",
    "    top_cats = Dict()\n",
    "    for feature in features\n",
    "        col_data = df[!, feature]\n",
    "        category_counts = StatsBase.countmap(col_data)\n",
    "        sorted_categories = sort(collect(category_counts), by=x->x[2], rev=true)\n",
    "        \n",
    "        # Take minimum between n and the number of unique categories\n",
    "        num_categories = min(n, length(sorted_categories))\n",
    "        \n",
    "        top_cats[feature] = [x[1] for x in sorted_categories[1:num_categories]]\n",
    "    end\n",
    "    return top_cats\n",
    "end\n",
    "\n",
    "# Get top 3 categories for specific features\n",
    "top_categories = get_top_categories(train_df, categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode only the top 3 categories\n",
    "function one_hot_top_categories!(df, top_categories)\n",
    "    for (feature, top_cats) in top_categories\n",
    "        if length(top_cats) == 2  # Handle the binary case\n",
    "            # Assuming the first category in top_cats is treated as 'true'\n",
    "            new_col_name = Symbol(string(feature, \"_binary\"))\n",
    "            df[!, new_col_name] = df[!, feature] .== top_cats[1]\n",
    "        else  # Handle the general case\n",
    "            for cat in top_cats\n",
    "                new_col_name = Symbol(string(feature, \"_\", cat))\n",
    "                df[!, new_col_name] = df[!, feature] .== cat\n",
    "            end\n",
    "        end\n",
    "        select!(df, Not(Symbol(feature)))  # Drop the original feature column\n",
    "    end\n",
    "end\n",
    "\n",
    "# Apply one-hot encoding\n",
    "one_hot_top_categories!(train_df, top_categories)\n",
    "\n",
    "# Serialize the top_categories dictionary to a binary file\n",
    "open(TOP_CATEGORIES, \"w\") do io\n",
    "    serialize(io, top_categories)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546-element Vector{UInt32}:\n",
       " 0x0000000d\n",
       " 0x0000000f\n",
       " 0x00000001\n",
       " 0x00000007\n",
       " 0x00000006\n",
       " 0x00000002\n",
       " 0x00000006\n",
       " 0x00000012\n",
       " 0x0000000d\n",
       " 0x00000012\n",
       " 0x00000004\n",
       " 0x00000007\n",
       " 0x00000010\n",
       "          ⋮\n",
       " 0x0000000d\n",
       " 0x00000002\n",
       " 0x00000007\n",
       " 0x00000010\n",
       " 0x00000010\n",
       " 0x0000000d\n",
       " 0x00000006\n",
       " 0x00000010\n",
       " 0x00000002\n",
       " 0x00000003\n",
       " 0x00000010\n",
       " 0x00000010"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_levels = levels(target)\n",
    "open(TARGET_LEVELS, \"w\") do io\n",
    "    serialize(io, target_levels)\n",
    "end\n",
    "\n",
    "target = categorical(target)\n",
    "encoded_target = int(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "We choose Random Forest Classifier model, but feel free to try your own and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Matrix(train_df[:, Not([id_feature])])\n",
    "\n",
    "# train random forest classifier\n",
    "# using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6\n",
    "model = build_forest(target, x_train, 2, 10, 0.5, 6)\n",
    "\n",
    "open(PREDICTOR_FILE_PATH, \"w\") do io\n",
    "    serialize(io, model)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546-element CategoricalArrays.CategoricalArray{String31,1,UInt32}:\n",
       " String31(\" frog-eye-leaf-spot\")\n",
       " String31(\" phyllosticta-leaf-spot\")\n",
       " String31(\" 2-4-d-injury\")\n",
       " String31(\" brown-stem-rot\")\n",
       " String31(\" brown-spot\")\n",
       " String31(\" alternarialeaf-spot\")\n",
       " String31(\" brown-spot\")\n",
       " String31(\" purple-seed-stain\")\n",
       " String31(\" frog-eye-leaf-spot\")\n",
       " String31(\" purple-seed-stain\")\n",
       " String31(\" bacterial-blight\")\n",
       " String31(\" brown-stem-rot\")\n",
       " String31(\" phytophthora-rot\")\n",
       " ⋮\n",
       " String31(\" frog-eye-leaf-spot\")\n",
       " String31(\" alternarialeaf-spot\")\n",
       " String31(\" brown-stem-rot\")\n",
       " String31(\" phytophthora-rot\")\n",
       " String31(\" phytophthora-rot\")\n",
       " String31(\" frog-eye-leaf-spot\")\n",
       " String31(\" brown-spot\")\n",
       " String31(\" phytophthora-rot\")\n",
       " String31(\" alternarialeaf-spot\")\n",
       " String31(\" anthracnose\")\n",
       " String31(\" phytophthora-rot\")\n",
       " String31(\" phytophthora-rot\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
